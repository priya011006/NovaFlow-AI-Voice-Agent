# ğŸ™ï¸ `AI Voice Agent â€“ Day 25 | 30 Days of Voice Agents Challenge ğŸš€`

Welcome to **Day 17 of my AI Voice Agent journey**, part of the #BuildWithMurf challenge! ğŸŒŸ

Todayâ€™s focus is on **real-time audio streaming and transcription** using **FastAPI, WebSockets, and AssemblyAI**. This feature allows users to **speak directly into their microphone** and receive **live transcription updates in the browser**. Itâ€™s a core step toward building **interactive, voice-powered AI assistants** that feel responsive and dynamic. ğŸ’¬

---

## ğŸ“– About Day 17

On Day 17, I implemented a **live audio transcription system** with the following capabilities:

* ğŸ¤ **Real-time Speech-to-Text (STT)** using AssemblyAI
* ğŸ”„ **WebSocket streaming** for instant transcription updates
* ğŸ’¾ **Recording and saving audio** in `.wav` format
* âš¡ **Error handling & retry mechanisms** for robust streaming
* ğŸ–¥ï¸ **Responsive front-end** for displaying live transcription

This setup lays the foundation for any **voice assistant**, enabling **immediate feedback** and **dynamic interaction**.

---

## ğŸ”‘ Key Features

* âœ… Start/Stop transcription directly from the Web UI
* âœ… Live transcription updates as you speak
* âœ… Save audio recordings in `uploads/` for later reference
* âœ… Retry mechanism for reconnecting WebSocket on failure
* âœ… Fully responsive UI â€“ works seamlessly on mobile and desktop
* âœ… Robust error handling for microphone and API issues

---

## ğŸ—ï¸ Architecture Overview

```text
[Browser UI]  â†’  WebSocket (ws://127.0.0.1:8000/ws)  â†’  [FastAPI Backend]
   â”‚                                             â”‚
   â”‚  Record audio                               â”‚  Receive audio chunks
   â”‚                                             â”‚
   â–¼                                             â–¼
[Transcription Box] â† AssemblyAI StreamingClient  â†’  Server sends partial & final transcripts
   â”‚
   â–¼
Display live transcription + save .wav file in uploads/
```

This architecture allows **real-time communication** between the browser and backend while continuously updating the transcription box.

---

## ğŸ“‚ Project Structure

```bash
day17-ai-voice-agent/
â”œâ”€ templates/
â”‚  â””â”€ index.html                # Frontend UI for transcription
â”œâ”€ static/
â”‚  â”œâ”€ style.css                 # CSS styles
â”‚  â””â”€ favicon.ico               # Browser icon
â”œâ”€ uploads/                      # Saved audio files (.wav)
â”œâ”€ main.py                       # FastAPI app + WebSocket + streaming logic
â”œâ”€ requirements.txt              # Python dependencies
â”œâ”€ .env                          # Environment variables (API key)
â””â”€ README.md                     # Project documentation
```

---

## ğŸš€ Step-by-Step Setup

Follow these steps to **run the Day 17 project locally**:

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/yourusername/day17-ai-voice-agent.git
cd day17-ai-voice-agent
```

### 2ï¸âƒ£ Create & Activate Python Virtual Environment

```bash
# Mac/Linux
python3 -m venv venv
source venv/bin/activate

# Windows
python -m venv venv
venv\Scripts\activate
```

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

**`requirements.txt` includes:**

```
fastapi==0.112.0
uvicorn[standard]==0.30.3
assemblyai>=0.36.0
pyaudio==0.2.14
ffmpeg-python==0.2.0
pydub==0.25.1
numpy==1.26.4
python-dotenv==1.0.1
```

### 4ï¸âƒ£ Set Up Environment Variables

Create a `.env` file in the project root with your **AssemblyAI API key**:

```ini
AAI_API_KEY=your_assemblyai_api_key
```

> âš ï¸ Make sure your API key is valid, otherwise the transcription will not work.

### 5ï¸âƒ£ Run the FastAPI Server

```bash
uvicorn main:app --reload
```

Server will run at:

```
http://127.0.0.1:8000
```

---

## ğŸ–¥ï¸ Usage Instructions

1. Open the browser at: `http://127.0.0.1:8000`
2. Click **Start Transcription ğŸµ** to begin streaming audio.
3. Speak into the microphone â€“ live transcription updates appear in the **transcription box**.
4. Click **Stop Transcription ğŸ›‘** to end recording. Audio is automatically saved in `uploads/`.
5. If the WebSocket disconnects, click **Retry ğŸ”„** to reconnect.

---

## ğŸ”§ Core Code Flow

### Frontend (`index.html + JS`)

* Handles:

  * WebSocket connection (`ws://127.0.0.1:8000/ws`)
  * Button events: start, stop, retry
  * Live transcription display
  * Error handling

### Backend (`main.py`)

* `ws_handler(websocket: WebSocket)`:

  * Accepts WebSocket connections
  * Streams audio chunks from PyAudio to AssemblyAI
  * Sends partial and final transcripts to the UI
  * Saves audio as `.wav` files in `uploads/`
  * Handles stop/start/retry commands

* `save_wav(frames)` â†’ Stores recorded audio locally

* `StreamingClient` â†’ AssemblyAI live transcription

---

## ğŸ“¸ UI Preview

**Transcription Box:**

```
[12:45:21] Hello, this is a live transcription ğŸ“œ
```

**Status Box:**

```
Status: Transcribing... ğŸ™ï¸
Server: Connected âœ…
```

---

## ğŸŒŸ Learning Outcomes

* Built **real-time speech-to-text** with minimal latency
* Learned **WebSocket integration** between frontend & backend
* Managed **threading, async tasks, and event-driven streaming**
* Developed a **responsive and interactive transcription UI**

---

## ğŸ“œ License

MIT License â€” Free to use, modify, and share ğŸš€

---

## ğŸ™Œ Acknowledgments

* **AssemblyAI** â€“ live transcription API
* **FastAPI** â€“ backend framework with WebSocket support
* **#BuildWithMurf** â€“ challenge inspiration & community support

---

âœ¨ `Follow my journey` â†’ [LinkedIn](https://www.linkedin.com/in/deepak-mallareddy-1b09b6274/)

#Day17 #VoiceAgent #AssemblyAI #WebSocket #AI #BuildWithMurf

---

